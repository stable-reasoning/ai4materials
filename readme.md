<p align="center">
 <!-- <img src="docs/logo.png" style="height: 80px;"> -->
 <h1 align="center">Improving LLM Reasoning in Materials Science</h1>
</p>
<p align="center">
 <a><img alt="Status" src="https://img.shields.io/badge/status-research_prototype-6a5acd"></a>
 <a><img alt="Python" src="https://img.shields.io/badge/Python-3.10%2B-blue"></a>
 <a><img alt="License" src="https://img.shields.io/badge/license-MIT-lightgrey"></a>
</p>

## About


This repository is a part of Master's thesis in AI: "Enhancing the Reasoning of LLMs in Materials Discovery via Causal Contracts"

## ğŸ“‹ Overview

This repository contains a modular, DAGâ€‘based framework for experimenting with **pipelines that test and improve the reasoning capabilities of large language models (LLMs)**. The system offers two endâ€‘toâ€‘end pipelines:

* **Document Pipeline** â€” downloads papers, extracts pages, performs multiâ€‘stage analysis, and synthesizes structured artifacts (semantic summaries and contracts) before generating QA datasets.
* **Answer & Evaluation Pipeline** â€” answers the generated questions under configurable context settings and evaluates performance with transparent, reproducible artifacts.

The pipelines are built from interoperable **agents** orchestrated by a lightweight **DAG runner**, making it easy to plug in new components, models, or evaluation strategies. Code quality, reproducibility, and documentation are priorities.


> ğŸ“– Paper: *coming soon*

## ğŸ Highlights

* **DAGâ€‘orchestrated research pipelines** with explicit data contracts between tasks.
* **Agentic design**: discrete agents for download, extraction, analysis, contract writing, QA generation, answering, and evaluation.
* **Reproducible runs**: each experiment writes all inputs/outputs to a timestamped folder under `runs/`.
* **Configurable model runtime** via CLI flags (`--model`, `--temperature`, `--retries`).
* **Context controls** for ablations (e.g., raw text vs. additional context flags).

## ğŸ“‹ Table of Contents

* [ğŸš€ Get Started](#-get-started)

  * [System Requirements](#system-requirements)
  * [Installation](#installation)
  * [Environment Setup](#environment-setup)
  * [Quick Start](#quick-start)
* [ğŸ”§ Framework Components](#-framework-components)

  * [ğŸ§¬ Document Pipeline](#-document-pipeline)
  * [ğŸ§ª Answer & Evaluation Pipeline](#-answer--evaluation-pipeline)
  * [âš™ï¸ Core Runtime](#ï¸-core-runtime)
  * [ğŸ—‚ï¸ Utilities & Middleware](#ï¸-utilities--middleware)
* [ğŸ“¦ Project Structure](#-project-structure)
* [ğŸ“ˆ Reproducibility & Experiment Artifacts](#-reproducibility--experiment-artifacts)
* [ğŸ“ Configuration Reference](#-configuration-reference)
* [ğŸ™Œ Acknowledgements](#-acknowledgements)
* [ğŸ“š Citation](#-citation)

## ğŸš€ Get Started

#### System Requirements

* Python **3.10+**
* macOS/Linux/WSL2 (Windows native should also work)
* Recommended: virtual environment (Conda/venv)

#### Installation

1. **Clone the repository**

```bash
git clone <your-repo-url>
cd <your-repo>
```

2. **Create and activate a Conda environment**

```bash
conda create -n llm-reasoning python=3.10 -y
conda activate llm-reasoning
```

3. **Install dependencies**

* Using pip (from `requirements.txt`):

```bash
pip install -r requirements.txt
```

* Or, if you maintain an `environment.yml`:

```bash
conda env update -n llm-reasoning -f environment.yml
```

> Tip: If you're on Apple Silicon or using CUDA, prefer installing any platformâ€‘specific packages via conda first, then fall back to pip.

#### Environment Setup

Minimal configuration is required for local runs. By default, artifacts are written to `./runs`. Ensure the directories under `data/` and `test_data/` exist and contain the input files referenced below.

#### Quick Start

**Document Pipeline** â€” given a list of paper URLs:

```bash
python app.py document  --working-dir runs \
  --papers ./test_data/papers_1.lst \
  --run-id doc_pipeline_test  \
  --model openai/o4-mini --temperature 1.0 --retries 3
```

**Answer & Evaluation Pipeline** â€” given a QA dataset and contracts:

```bash
python app.py answer --working-dir runs \
  --contracts ./data/contracts2.json \
  --dataset   ./data/full_dataset2.json \
  --flags RAW_TEXT \
  --model openai/o4-mini --temperature 1.0 --retries 3
```

```bash
python app.py design  --working-dir runs \
   --contracts ./data/contracts2.json \
   --contract_id 21-0 \
   --run-id design_pipeline-20250915-065507 \
   --model openai/o4-mini --temperature 1.0 --retries 3

```

> Both commands create a timestamped experiment folder inside `runs/` that contains all intermediate and final artifacts.

## ğŸ”§ Framework Components

### General notes on code architecture

The LLM client part is implemented using LiteLLM library, and this makes our tool model-agnostic. When using two providers,
appropriate API keys must be provided. 

### ğŸ§¬ Document Pipeline

**Goal:** transform raw papers into structured, analysisâ€‘ready artifacts and generate a highâ€‘quality QA dataset for reasoning evaluation.

**Stages (agents):**

1. **DownloadAgent** â€” fetches documents from URLs listed in `--papers`.
2. **ExtractionAgent** â€” extracts pages and creates normalized document IDs.
3. **DocumentAnalyzerAgent** â€” runs firstâ€‘pass analysis on the processed documents.
4. **SemanticAnalyzerAgent** â€” performs deeper semantic analysis to derive structured representations.
5. **ContractWriterAgent** â€” synthesizes *contracts* (concise, schemaâ€‘constrained summaries) to support controlled QA.
6. **QADatasetGeneratorAgent** â€” produces a QA dataset aligned with the contracts and selected context flags.

**Outputs:** processed IDs, semantic documents, contracts, and a generated QA dataset â€” all stored under the current run directory.


### ğŸ§ª Answer & Evaluation Pipeline

**Goal:** answer questions under specified context settings and evaluate performance.

**Stages (agents):**

1. **QAAnswerAgent** â€” answers questions from a QA dataset using the selected model configuration and context flags.
2. **QAEvaluationAgent** â€” computes evaluation metrics and produces analysis artifacts.

**Outputs:** model answers and evaluation reports (JSON/CSV/plots as configured) under the current run directory.

### âš™ï¸ Core Runtime

* **DAG / DAGRunner** â€” schedules and executes agents given explicit data dependencies; ensures artifact lineage, reproducibility, and fault isolation.
* **Composability** â€” use the DAG builders in `app4.py` as examples to assemble new experimental pipelines.

### ğŸ—‚ï¸ Utilities & Middleware

* **`utils.prompt_manager.PromptManager`** â€” centralizes prompts used by analysis/answering agents.
* **`utils.common.ModelConfig`** â€” encapsulates model runtime knobs (`name`, `temperature`, `retries`).
* **`middleware.ImageStorage`** â€” manages image assets produced along the pipeline.

## ğŸ“¦ Project Structure


```
<repo-root>/
â”œâ”€ agents/                  # Agent implementations (download, extract, analyze, QA, eval, ...)
â”œâ”€ core/                    # DAG & runtime abstractions
â”œâ”€ data/                    # Generated artifacts (contracts, datasets, etc)
â”œâ”€ docs/                    # Technical documentation
â”œâ”€ docucache/              
â”œâ”€ middleware/              # Shared services (e.g., image storage)
â”œâ”€ notebooks/               # Analytics for thesis            
â”œâ”€ runs/                    # Experiment outputs (created at runtime)
â”œâ”€ test_data/               # Test lists / small fixtures (e.g., paper URLs)
â”œâ”€ utils/                   # Prompt manager, settings, model config, logging helpers
â”œâ”€ requirements.txt         # Python dependencies
â””â”€ app.py                  # CLI entrypoint: build & run DAG pipelines
```

## The file structure of DocuCache

```shell
â””â”€â”€ docucache/
    â”œâ”€â”€ metadata.db            <-- The SQLite database file
    â”œâ”€â”€ 1/                     <-- First paper's folder (ID from DB)
    â”‚   â”œâ”€â”€ assets/
    â”‚   â””â”€â”€ tmp/
    â”‚       â””â”€â”€ 1706.03762.pdf
    â”œâ”€â”€ 2/
    â”‚   â”œâ”€â”€ assets/
    â”‚   â””â”€â”€ tmp/
    â”‚       â””â”€â”€ 2203.02155.pdf
    â””â”€â”€ 3/
        â”œâ”€â”€ assets/
        â””â”€â”€ tmp/
            â””â”€â”€ 2307.09288.pdf
```



## ğŸ“ˆ Reproducibility & Experiment Artifacts

Every run produces a folder `runs/<pipeline-name>-YYYYMMDD-HHMMSS/` containing:

* **Inputs:** exact copies or references to all inputs used by each agent.
* **Intermediate Artifacts:** JSON manifests (e.g., processed document IDs, semantic docs, contracts, model answers).
* **Final Reports:** evaluation summaries and any generated plots/tables.
* **Logs:** structured logs with timestamps for traceability.

You can pass a custom `--run-id` to name the experiment folder deterministically.

## ğŸ“ Configuration Reference

### Common CLI Flags

* `--working-dir` (default: `runs`) â€” base directory for artifacts.
* `--model` (default: `openai/o4-mini`) â€” model name/ID in a standard format provider/model_name.
* `--temperature` (default: `1.0`) â€” sampling temperature.
* `--retries` (default: `3`) â€” retry attempts for model calls.

### Document Pipeline

* `--papers` â€” path to a text file containing one URL per line (default: `./test_data/papers_1.lst`).
* `--run-id` â€” optional experiment/run identifier.

### Answer & Evaluation Pipeline

* `--dataset` â€” path to a QA dataset JSON (default: `./data/test_dataset.json`).
* `--contracts` â€” path to a contracts JSON (default: `./data/test_contracts.json`).
* `--flags` â€” context flags (e.g., `RAW_TEXT`, `CC`, or `CC+RAW_TEXT`).
* `--run-id` â€” optional experiment/run identifier.

### Programmatic Use

Import and extend the DAG builders from `app.py`:

```python
from app import get_document_pipeline_dag, get_answer_pipeline_dag

# Build a custom DAG and run it with your own runner/configuration
```

## ğŸ“š Citation

If you use this repository in academic work, please cite it as:

```bibtex
@misc{kaliutau2025,
  author = {Kaliutau, A.},
  title = {{AI in Material Science}},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/stable-reasoning/ai-material-science}},
}
```

---

### ğŸ’¡ Tips

* Keep the structure of your input files stable; the agents rely on consistent schemas.
* For ablations, duplicate a prior run with only one change (e.g., `--flags`, `--temperature`).
* Use `--run-id` to align artifacts across multiple pipelines when running a batched study.

