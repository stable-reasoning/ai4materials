You are an expert AI assistant specializing in scientific reasoning.
Are are given the causal contract describing the complex mechanism of some concept/phenomenon generated from article (contract_5.json)
Your task is to generate on its basis a test dataset of as many as possible
Question-Answer pairs to test the reasoning capabilities of AI, in particular its ability to relate complex concepts and do multi-hop reasoning.

we want to show that using causal contracts like above (contract_5.json) improves reasoning capabilities in the sense that allows to answer MORE COMPLEX questions, that using raw text (ie. rec_5.json). It could be: multi-hop questions  generated from hypothesis, or testing how many causal links are restored, or fill-gap questions for predictions or other parts of contract. Generate the examples of such questions. Before offering them, internally validate the answers based on raw text mostly fail

GOAL

Produce N complex questions (default N=12 unless a different N is specified) such that:

Each question is easy using the causal contract (because the required causal chain, assumptions, predictions, or rubric entries are explicit there),

but hard for naïve extractive QA over the raw text (because facts are scattered, implicit, or require multi-hop synthesis).

You must internally validate each question with a lightweight raw-text coverage audit

QUESTION TYPES (mix across these; ensure diversity)

Generate a balanced set spanning at least 4 categories; no category should exceed 40% of items.

Multi-hop causal chain (3–5 hops)
Ask for a minimal path or stitched explanation from cause→…→effect that requires named intermediate mechanisms plus one or more numeric or state-identity details drawn from predictions/rubric.

Causal-graph restoration / edge typing
Ask to list ordered (cause, action_verb, effect) triples; or to reconstruct a minimal DAG for a given intervention. Include the exact verbs as constraints.

Fill-the-gap (assumptions / predictions / rubric)
Create cloze-style prompts where the blanks must be filled by assumptions, predictions (including specific verbs), or rubric-mapped claims.

Counterfactual / assumption stress-test
Perturb a key assumption (e.g., break clean-limit, flip a field orientation), and ask what parts of the causal chain would fail and why.

Multi-constraint retrieval
Require identity + threshold + mechanism together (e.g., “which state is induced above ~X, with PVR ~Y, and why it’s not due to mechanism Z”), combining categorical and numeric constraints.

Cross-section: At least 50% of items must combine two or more of: field tuning, SOC proximity, cleanliness/transport regime, parent-phase identity, Pauli-limit metrics, or topological coexistence.

Numbers: ≥50% of items must require numeric ranges or thresholds (with units) present in contract predictions/rubric.

OUTPUT FORMAT (JSONL; one object per line)

Each line must be a JSON object with exactly these fields:

id: string (unique, slug-like)

category: one of ["multi_hop","graph_restoration","fill_gap","counterfactual","multi_constraint"]

question: string (the prompt shown to a human/model)

gold_answer: object

Must be solvable purely from the contract (no hidden reasoning).

Include: structured fields like path, triples, fills, nodes, edges, state, numeric_values, etc. Use only facts explicitly present in the contract.

gold_references: object

Pointers into CONTRACT_JSON: e.g., hypothesis_indices, edge_triplets (verbatim triples), assumptions_used, predictions_used, rubric_refs (e.g., source_text indices or figure refs if present).

Do not add explanations—just references/IDs/strings from the contract.

scoring: object

max_score: integer

must_include: array of case-insensitive strings the predicted answer must contain (terms, units, state names, action verbs).

Optional numeric_ranges: array of { "name": str, "min": num, "max": num, "units": str }.

Optional structure_checks: array of simple checks, e.g.
{"check":"mentions_chain_order","keywords":[["termA","termB"],["termC","termD"]]}
Interpreted as: each inner list must co-occur for credit; award remaining points if all groups are satisfied.

raw_text_validation: object

single_chunk_covers_all: boolean

term_hit_pages: map (term → list<int>)

any_chunk_pages: list<int>

notes: short string identifying why this is trivial via the contract but non-trivial via the raw text (no reasoning, just a one-line characterization, e.g., “requires 4-hop chain + verb labels + cross-condition numbers scattered across sections”).

No other fields are allowed. Do not include chain-of-thought, derivations, or justifications beyond the fields above.

Answers should be yes-no, multichoice or 3-word sentence

STRICTLY use the following formatting rules for the output:
- The type of questions: fill the gap
- Use claims in "predictions" section of contract and identify ONE or TWO main key word/PAIR of words which can be hidden (information masking). After hiding the sentence should loose it's original prediction meaning.
- Each claim in "predictions" section may result in one variant of question. Prefer masking the numerical values, science terms/special words in the sentence which are difficult to guess without reasoning and multiple options evaluation.
- Questions must be self-contained. Use only the provided contract (no external knowledge, no web search) and domain knowledge for the topic.
- If there are special symbols in text (equations, chemical formulas, etc), use simple latex notation for these words. Copy these words directly from the source.
- Return JSON ONLY.

Output schema (on the example of one block):

[{
  "contract_no": "int - use order number of contract in the list",
  "justification": "string - explain in short what we are hiding",
  "hidden_words": "List[string] - the list of exact words in prediction to hide",
  "orig_prediction": "string - copy it from the original contract",
}]

